{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb5c854-b0a4-435f-9108-df7329b3dd51",
   "metadata": {
    "tags": []
   },
   "source": [
    "![alt text](metrics_phenotyping.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a02078-96b3-4f14-9669-fb49f4edb4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import  balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289ba758-e261-48fa-ac80-f8d81e62fce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 4060 Ti')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c014230b-cda3-4cbc-920a-2d5b16570858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from net import GNNStack\n",
    "from utils_todynet_multiclass import AverageMeter, accuracy, log_msg, get_default_train_val_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5d7406-8f67-45bd-add7-cb4a8fe7a97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a458334-b4e5-4c9b-bacb-976cb10ec9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, alpha=None, gamma=2):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608f5b7-259a-4fea-be59-b90be979c3b9",
   "metadata": {},
   "source": [
    "## aruguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e2de2d-8c0d-4ab8-8090-5e5621f13034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'arch': 'dyGIN2d', #what other models I can put here?? dyGCN2d, dyGIN2d\n",
    "    'dataset': 'Phenotyping', # \"AtrialFibrillation\" # 'Mortality', # 'MIMIC3'\n",
    "    'num_layers': 2,  # the number of GNN layers  3\n",
    "    'groups': 16,  # the number of time series groups (num_graphs)\n",
    "    'pool_ratio': 0.1,  # the ratio of pooling for nodes\n",
    "    'kern_size': [3,3],  # list of time conv kernel size for each layer [9,5,3]\n",
    "    'in_dim': 32,  # input dimensions of GNN stacks\n",
    "    'hidden_dim': 32,  # hidden dimensions of GNN stacks\n",
    "    'out_dim': 32,  # output dimensions of GNN stacks\n",
    "    'workers': 0,  # number of data loading workers\n",
    "    'epochs': 30,  # number of total epochs to run\n",
    "    'batch_size': 4,  # mini-batch size, this is the total batch size of all GPUs\n",
    "    'val_batch_size': 4,  # validation batch size\n",
    "    'lr': 0.000095,  # initial learning rate\n",
    "    'weight_decay': 1e-4,  # weight decay\n",
    "    'evaluate': False,  # evaluate model on validation set\n",
    "    'seed': 42,  # seed for initializing training\n",
    "    'gpu': 0,  # GPU id to use\n",
    "    'use_benchmark': True,  # use benchmark\n",
    "    'tag': 'date',  # the tag for identifying the log and model files\n",
    "    'weight':'alldata'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982150c8-5c87-471b-b810-75f52df10c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshashankyadav321\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shashank/data/hirid_setup/todynet/TodyNet/wandb/run-20240213_084146-j9gffq31</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shashankyadav321/phenotyping/runs/j9gffq31' target=\"_blank\">virtuous-rat-1</a></strong> to <a href='https://wandb.ai/shashankyadav321/phenotyping' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shashankyadav321/phenotyping' target=\"_blank\">https://wandb.ai/shashankyadav321/phenotyping</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shashankyadav321/phenotyping/runs/j9gffq31' target=\"_blank\">https://wandb.ai/shashankyadav321/phenotyping/runs/j9gffq31</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/shashankyadav321/phenotyping/runs/j9gffq31?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff3115851d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"phenotyping\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c31c8a0-dcb1-4e0c-b44e-36bb9bd8f4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TensorDataset(data_train, label_train)\n",
    "# val_dataset   = TensorDataset(data_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f6c801-7a9f-4f8a-9cbf-362febcf0683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=args['batch_size'],shuffle=True, num_workers=args['workers'], pin_memory=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['val_batch_size'], shuffle=False,num_workers=args['workers'],pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a648e31-70d5-4f32-b03a-6fd95b5211e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # args = parser.parse_args()\n",
    "    \n",
    "#     # args.kern_size = [ int(l) for l in args.kern_size.split(\",\") ]\n",
    "\n",
    "#     # if args.seed is not None:\n",
    "#     random.seed(args['seed'])\n",
    "#     torch.manual_seed(args['seed'])\n",
    "\n",
    "#     main_work(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "991c1bc0-5e2d-4ea2-9691-3e2fac259bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_work(args):\n",
    "    \n",
    "    random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    \n",
    "    \n",
    "    # init acc\n",
    "    best_acc1 = 0\n",
    "    best_ba  = 0\n",
    "    # best_pr   = 0\n",
    "    \n",
    "    if args['tag'] == 'date':\n",
    "        local_date = time.strftime('%m.%d %H:%M', time.localtime(time.time()))\n",
    "        args['tag'] = local_date\n",
    "\n",
    "    log_file = 'log/{}_gpu{}_{}_{}_exp.txt'.format(args['tag'], args['gpu'], args['arch'], args['dataset'])\n",
    "    \n",
    "    \n",
    "    if args['gpu'] is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args['gpu']))\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    train_loader, val_loader, num_nodes, seq_length, num_classes = get_default_train_val_test_loader(args)\n",
    "    \n",
    "    print('features / nodes', num_nodes,'total time graphs',seq_length,'classes', num_classes)\n",
    "    \n",
    "    # training model from net.py\n",
    "    model = GNNStack(gnn_model_type=args['arch'], num_layers=args['num_layers'], \n",
    "                     groups=args['groups'], pool_ratio=args['pool_ratio'], kern_size=args['kern_size'], \n",
    "                     in_dim=args['in_dim'], hidden_dim=args['hidden_dim'], out_dim=args['out_dim'], \n",
    "                     seq_len=seq_length, num_nodes=num_nodes, num_classes=num_classes)\n",
    "\n",
    "    # print & log\n",
    "    log_msg('epochs {}, lr {}, weight_decay {}'.format(args['epochs'], args['lr'], args['weight_decay']), log_file)\n",
    "    \n",
    "    log_msg(str(args), log_file)\n",
    "\n",
    "\n",
    "    # determine whether GPU or not\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Warning! Using CPU!!!\")\n",
    "    elif args['gpu'] is not None:\n",
    "        torch.cuda.set_device(args['gpu'])\n",
    "\n",
    "        # collect cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = model.cuda(args['gpu'])\n",
    "        if args['use_benchmark']:\n",
    "            cudnn.benchmark = True\n",
    "        print('Using cudnn.benchmark.')\n",
    "    else:\n",
    "        print(\"Error! We only have one gpu!!!\")\n",
    "\n",
    "\n",
    "    # define loss function(criterion) and optimizer\n",
    "    weights = np.array([1.48,2.15,3.77,1.0,4.51,3.54,10.20,61.57,1.90,14.27,4.93,3.17,23.72,130.0,19.39])  ### according to whole data\n",
    "    # weights = np.array([1.44,2.15,3.71,1.00,4.54,3.56,9.90,58.43,1.93,14.10,4.97,3.15,20.97,106.70,18.88]) ### according to train\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).cuda(args['gpu'])   \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).cuda(args['gpu'])\n",
    "    # criterion = nn.CrossEntropyLoss().cuda(args['gpu'])\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    # validation\n",
    "    if args['evaluate']:\n",
    "        validate(val_loader, model, criterion, args)\n",
    "        return\n",
    "\n",
    "    # train & valid\n",
    "    print('****************************************************')\n",
    "    print('Dataset: ', args['dataset'])\n",
    "\n",
    "    dataset_time = AverageMeter('Time', ':6.3f')\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    epoches = []\n",
    "    \n",
    "    ###### 4 more lists to have values\n",
    "    ba_train = []\n",
    "    # pr_train  = []\n",
    "    \n",
    "    ba_val   = []\n",
    "    # pr_val    = []\n",
    "\n",
    "    end = time.time()\n",
    "    for epoch in tqdm(range(args['epochs'])):\n",
    "        epoches += [epoch]\n",
    "\n",
    "        # train for one epoch\n",
    "        acc_train_per, loss_train_per, output_train_per, target_train_per = train(train_loader, model, criterion, optimizer, lr_scheduler, args)\n",
    "        \n",
    "        acc_train += [acc_train_per]\n",
    "        loss_train += [loss_train_per]\n",
    "        # calculate metric\n",
    "        # print(len(target_train_per),len(output_train_per))\n",
    "        \n",
    "        # ohe_train = torch.nn.functional.one_hot(target_train_per, num_classes=15)\n",
    "        \n",
    "        ba_value_train = balanced_accuracy_score(target_train_per, output_train_per)\n",
    "        # auc_pr_value_train = average_precision_score(target_train_per, output_train_per)\n",
    "        #new code\n",
    "        ba_train += [ba_value_train]\n",
    "        # pr_train  += [auc_pr_value_train]\n",
    "\n",
    "        msg = f'TRAIN, epoch {epoch}, train_loss {loss_train_per}, train_bacc {ba_value_train}'\n",
    "\n",
    "        print(f'TRAIN, epoch {epoch}, train_loss {loss_train_per:.5f}, train_bacc {ba_value_train:.5f}')\n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        acc_val_per, loss_val_per, output_val_per, target_val_per = validate(val_loader, model, criterion, args)\n",
    "\n",
    "        acc_val  += [acc_val_per]\n",
    "        loss_val += [loss_val_per]\n",
    "        #calculate metric\n",
    "        # calculate metric\n",
    "        # print(len(target_val_per),len(output_val_per))\n",
    "        ba_value_val = balanced_accuracy_score(target_val_per, output_val_per)\n",
    "        # auc_pr_value_val = average_precision_score(target_val_per, output_val_per)\n",
    "        #new code\n",
    "\n",
    "        msg = f'VAL, epoch {epoch}, val_loss {loss_val_per}, val_bacc {ba_value_val}'\n",
    "        \n",
    "        print(f'VAL, epoch {epoch}, val_loss {loss_val_per:.5f}, val_bacc {ba_value_val:.5f}')\n",
    "        \n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "        \n",
    "        \n",
    "        # remember best acc\n",
    "        best_acc1 = max(acc_val_per, best_acc1)\n",
    "        \n",
    "        best_ba = max(ba_value_val, best_ba)\n",
    "        \n",
    "        # best_pr  = max(auc_pr_value_val, best_pr)\n",
    "        \n",
    "        wandb.log({\"train_loss\": loss_train_per, \"train_ba\": ba_value_train, \"val_loss\": loss_val_per, \"val_ba\": ba_value_val, \"best_val_ba\": best_ba})\n",
    "        \n",
    "    wandb.finish()\n",
    "    # measure elapsed time\n",
    "    dataset_time.update(time.time() - end)\n",
    "\n",
    "    # log & print the best_acc\n",
    "    msg = f'\\n\\n * BEST_ACC: {best_acc1}\\n * TIME: {dataset_time}\\n'\n",
    "    log_msg(msg, log_file)\n",
    "\n",
    "    print(f' * best_acc1: {best_acc1}, best_ba: {best_ba}')\n",
    "    print(f' * time: {dataset_time}')\n",
    "    print('****************************************************')\n",
    "\n",
    "\n",
    "    # collect cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, lr_scheduler, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    \n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for count, (data, label) in enumerate(train_loader):\n",
    "\n",
    "        # data in cuda\n",
    "        data = data.cuda(args['gpu']).type(torch.float)\n",
    "        label = label.cuda(args['gpu']).type(torch.long)\n",
    "\n",
    "        # compute output\n",
    "        output = model(data)\n",
    "        \n",
    "        # print(output, label)\n",
    "    \n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1 = accuracy(output, label, topk=(1, 1))\n",
    "        \n",
    "        output_np = np.argmax(torch.softmax(output, dim=1).detach().cpu().numpy(), axis=1).tolist()\n",
    "        \n",
    "        target_np = label.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # print(output_np, target_np)\n",
    "        # break\n",
    "        \n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(acc1[0], data.size(0))\n",
    "        \n",
    "        # met_roc.update(roc, data.size(0))\n",
    "        # met_pr.update(pr, data.size(0))\n",
    "        output_list += output_np\n",
    "        target_list += target_np\n",
    "\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    lr_scheduler.step(top1.avg)\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (data, label) in enumerate(val_loader):\n",
    "            if args['gpu'] is not None:\n",
    "                data = data.cuda(args['gpu'], non_blocking=True).type(torch.float)\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda(args['gpu'], non_blocking=True).type(torch.long)\n",
    "\n",
    "            # compute output\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            output_np = np.argmax(torch.softmax(output, dim=1).detach().cpu().numpy(), axis=1).tolist()\n",
    "            target_np = label.detach().cpu().numpy().tolist()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1 = accuracy(output, label, topk=(1, 1))\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(acc1[0], data.size(0))\n",
    "            \n",
    "            output_list += output_np\n",
    "            target_list += target_np\n",
    "            \n",
    "            # met_roc.update(roc, data.size(0))\n",
    "            # met_pr.update(pr, data.size(0))\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cb92e-c573-4e83-a10e-791712ca78d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "features / nodes 231 total time graphs 288 classes 15\n",
      "Using cudnn.benchmark.\n",
      "****************************************************\n",
      "Dataset:  Phenotyping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585f366b3f544e2fb70b3bc3720b9d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN, epoch 0, train_loss 2.50814, train_bacc 0.11819\n",
      "VAL, epoch 0, val_loss 2.23589, val_bacc 0.16281\n",
      "TRAIN, epoch 1, train_loss 2.30127, train_bacc 0.15800\n",
      "VAL, epoch 1, val_loss 2.16416, val_bacc 0.17497\n",
      "TRAIN, epoch 2, train_loss 2.21400, train_bacc 0.17700\n",
      "VAL, epoch 2, val_loss 2.01315, val_bacc 0.21130\n",
      "TRAIN, epoch 3, train_loss 2.15260, train_bacc 0.19544\n",
      "VAL, epoch 3, val_loss 1.97251, val_bacc 0.22864\n",
      "TRAIN, epoch 4, train_loss 2.10911, train_bacc 0.20672\n",
      "VAL, epoch 4, val_loss 1.90936, val_bacc 0.25665\n",
      "TRAIN, epoch 5, train_loss 2.04112, train_bacc 0.23312\n",
      "VAL, epoch 5, val_loss 1.93507, val_bacc 0.24104\n",
      "TRAIN, epoch 6, train_loss 1.99579, train_bacc 0.24713\n",
      "VAL, epoch 6, val_loss 1.78394, val_bacc 0.31169\n",
      "TRAIN, epoch 7, train_loss 1.95698, train_bacc 0.26932\n",
      "VAL, epoch 7, val_loss 1.76622, val_bacc 0.31326\n",
      "TRAIN, epoch 8, train_loss 1.91306, train_bacc 0.28115\n",
      "VAL, epoch 8, val_loss 1.74959, val_bacc 0.32288\n",
      "TRAIN, epoch 9, train_loss 1.87128, train_bacc 0.29140\n",
      "VAL, epoch 9, val_loss 1.70996, val_bacc 0.32610\n",
      "TRAIN, epoch 10, train_loss 1.83557, train_bacc 0.30334\n",
      "VAL, epoch 10, val_loss 1.64636, val_bacc 0.33802\n",
      "TRAIN, epoch 11, train_loss 1.78947, train_bacc 0.31856\n",
      "VAL, epoch 11, val_loss 1.64322, val_bacc 0.34392\n",
      "TRAIN, epoch 12, train_loss 1.76006, train_bacc 0.32107\n",
      "VAL, epoch 12, val_loss 1.63912, val_bacc 0.33969\n",
      "TRAIN, epoch 13, train_loss 1.72204, train_bacc 0.33620\n",
      "VAL, epoch 13, val_loss 1.64183, val_bacc 0.34288\n",
      "TRAIN, epoch 14, train_loss 1.68510, train_bacc 0.34159\n",
      "VAL, epoch 14, val_loss 1.62154, val_bacc 0.34569\n",
      "TRAIN, epoch 15, train_loss 1.63101, train_bacc 0.35719\n",
      "VAL, epoch 15, val_loss 1.60725, val_bacc 0.34650\n",
      "TRAIN, epoch 16, train_loss 1.59487, train_bacc 0.36332\n",
      "VAL, epoch 16, val_loss 1.62420, val_bacc 0.35036\n",
      "TRAIN, epoch 17, train_loss 1.57571, train_bacc 0.38272\n",
      "VAL, epoch 17, val_loss 1.60966, val_bacc 0.36169\n",
      "TRAIN, epoch 18, train_loss 1.53786, train_bacc 0.38423\n",
      "VAL, epoch 18, val_loss 1.59265, val_bacc 0.36601\n",
      "TRAIN, epoch 19, train_loss 1.50382, train_bacc 0.40267\n",
      "VAL, epoch 19, val_loss 1.61679, val_bacc 0.37488\n",
      "TRAIN, epoch 20, train_loss 1.45702, train_bacc 0.41922\n",
      "VAL, epoch 20, val_loss 1.59226, val_bacc 0.36005\n",
      "TRAIN, epoch 21, train_loss 1.43131, train_bacc 0.41710\n",
      "VAL, epoch 21, val_loss 1.61522, val_bacc 0.36043\n",
      "TRAIN, epoch 22, train_loss 1.38913, train_bacc 0.45526\n",
      "VAL, epoch 22, val_loss 1.63777, val_bacc 0.39701\n",
      "TRAIN, epoch 23, train_loss 1.35026, train_bacc 0.46414\n",
      "VAL, epoch 23, val_loss 1.61802, val_bacc 0.36451\n"
     ]
    }
   ],
   "source": [
    "main_work(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7b3fb-cdbe-463e-9930-3e07a5d373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ignite metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee5b71-df18-40ce-8306-46d25381df1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04268a-6e56-4c9a-af62-ef7a51a2c51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b59bea-155e-48ff-ad28-144da52e2b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d89b1-02ae-4bde-855d-9a7afb3fb29d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de46b36-bbed-4648-b307-08e36812fa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd294a-20b3-4102-a719-ad53f428a282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b77c3-2735-4cb4-b2df-515ec8cdf3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
