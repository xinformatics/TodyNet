{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb5c854-b0a4-435f-9108-df7329b3dd51",
   "metadata": {
    "tags": []
   },
   "source": [
    "![alt text](metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a02078-96b3-4f14-9669-fb49f4edb4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# import wandb\n",
    "\n",
    "from sklearn.metrics import  balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289ba758-e261-48fa-ac80-f8d81e62fce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce GTX 1650')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c014230b-cda3-4cbc-920a-2d5b16570858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from net import GNNStack\n",
    "from utils_todynet_multiclass import AverageMeter, accuracy, log_msg, get_default_train_val_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5d7406-8f67-45bd-add7-cb4a8fe7a97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a458334-b4e5-4c9b-bacb-976cb10ec9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, alpha=None, gamma=2):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608f5b7-259a-4fea-be59-b90be979c3b9",
   "metadata": {},
   "source": [
    "## aruguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e2de2d-8c0d-4ab8-8090-5e5621f13034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'arch': 'dyGIN2d', #what other models I can put here?? dyGCN2d, dyGIN2d\n",
    "    'dataset': 'Phenotyping', # \"AtrialFibrillation\" # 'Mortality', # 'MIMIC3'\n",
    "    'num_layers': 2,  # the number of GNN layers  3\n",
    "    'groups': 32,  # the number of time series groups (num_graphs)\n",
    "    'pool_ratio': 0.1,  # the ratio of pooling for nodes\n",
    "    'kern_size': [3,3],  # list of time conv kernel size for each layer [9,5,3]\n",
    "    'in_dim': 32,  # input dimensions of GNN stacks\n",
    "    'hidden_dim': 32,  # hidden dimensions of GNN stacks\n",
    "    'out_dim': 32,  # output dimensions of GNN stacks\n",
    "    'workers': 0,  # number of data loading workers\n",
    "    'epochs': 30,  # number of total epochs to run\n",
    "    'batch_size': 4,  # mini-batch size, this is the total batch size of all GPUs\n",
    "    'val_batch_size': 4,  # validation batch size\n",
    "    'lr': 0.0005,  # initial learning rate\n",
    "    'weight_decay': 1e-4,  # weight decay\n",
    "    'evaluate': False,  # evaluate model on validation set\n",
    "    'seed': 42,  # seed for initializing training\n",
    "    'gpu': 0,  # GPU id to use\n",
    "    'use_benchmark': True,  # use benchmark\n",
    "    'tag': 'date',  # the tag for identifying the log and model files\n",
    "    'weight':'alldata'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982150c8-5c87-471b-b810-75f52df10c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"phenotyping\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config=args\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c31c8a0-dcb1-4e0c-b44e-36bb9bd8f4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TensorDataset(data_train, label_train)\n",
    "# val_dataset   = TensorDataset(data_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f6c801-7a9f-4f8a-9cbf-362febcf0683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=args['batch_size'],shuffle=True, num_workers=args['workers'], pin_memory=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['val_batch_size'], shuffle=False,num_workers=args['workers'],pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a648e31-70d5-4f32-b03a-6fd95b5211e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # args = parser.parse_args()\n",
    "    \n",
    "#     # args.kern_size = [ int(l) for l in args.kern_size.split(\",\") ]\n",
    "\n",
    "#     # if args.seed is not None:\n",
    "#     random.seed(args['seed'])\n",
    "#     torch.manual_seed(args['seed'])\n",
    "\n",
    "#     main_work(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "991c1bc0-5e2d-4ea2-9691-3e2fac259bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_work(args):\n",
    "    \n",
    "    random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    \n",
    "    \n",
    "    # init acc\n",
    "    best_acc1 = 0\n",
    "    best_ba  = 0\n",
    "    # best_pr   = 0\n",
    "    \n",
    "    if args['tag'] == 'date':\n",
    "        local_date = time.strftime('%m.%d %H:%M', time.localtime(time.time()))\n",
    "        args['tag'] = local_date\n",
    "\n",
    "    log_file = 'log/{}_gpu{}_{}_{}_exp.txt'.format(args['tag'], args['gpu'], args['arch'], args['dataset'])\n",
    "    \n",
    "    \n",
    "    if args['gpu'] is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args['gpu']))\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    train_loader, val_loader, num_nodes, seq_length, num_classes = get_default_train_val_test_loader(args)\n",
    "    \n",
    "    print('features / nodes', num_nodes,'total time graphs',seq_length,'classes', num_classes)\n",
    "    \n",
    "    # training model from net.py\n",
    "    model = GNNStack(gnn_model_type=args['arch'], num_layers=args['num_layers'], \n",
    "                     groups=args['groups'], pool_ratio=args['pool_ratio'], kern_size=args['kern_size'], \n",
    "                     in_dim=args['in_dim'], hidden_dim=args['hidden_dim'], out_dim=args['out_dim'], \n",
    "                     seq_len=seq_length, num_nodes=num_nodes, num_classes=num_classes)\n",
    "\n",
    "    # print & log\n",
    "    log_msg('epochs {}, lr {}, weight_decay {}'.format(args['epochs'], args['lr'], args['weight_decay']), log_file)\n",
    "    \n",
    "    log_msg(str(args), log_file)\n",
    "\n",
    "\n",
    "    # determine whether GPU or not\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Warning! Using CPU!!!\")\n",
    "    elif args['gpu'] is not None:\n",
    "        torch.cuda.set_device(args['gpu'])\n",
    "\n",
    "        # collect cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = model.cuda(args['gpu'])\n",
    "        if args['use_benchmark']:\n",
    "            cudnn.benchmark = True\n",
    "        print('Using cudnn.benchmark.')\n",
    "    else:\n",
    "        print(\"Error! We only have one gpu!!!\")\n",
    "\n",
    "\n",
    "    # define loss function(criterion) and optimizer\n",
    "    weights = np.array([1.48,2.15,3.77,1.0,4.51,3.54,10.20,61.57,1.90,14.27,4.93,3.17,23.72,130.0,19.39])  ### according to whole data\n",
    "    # weights = np.array([1.44,2.15,3.71,1.00,4.54,3.56,9.90,58.43,1.93,14.10,4.97,3.15,20.97,106.70,18.88]) ### according to train\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float).cuda(args['gpu'])   \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).cuda(args['gpu'])\n",
    "    # criterion = nn.CrossEntropyLoss().cuda(args['gpu'])\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    # validation\n",
    "    if args['evaluate']:\n",
    "        validate(val_loader, model, criterion, args)\n",
    "        return\n",
    "\n",
    "    # train & valid\n",
    "    print('****************************************************')\n",
    "    print('Dataset: ', args['dataset'])\n",
    "\n",
    "    dataset_time = AverageMeter('Time', ':6.3f')\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    epoches = []\n",
    "    \n",
    "    ###### 4 more lists to have values\n",
    "    ba_train = []\n",
    "    # pr_train  = []\n",
    "    \n",
    "    ba_val   = []\n",
    "    # pr_val    = []\n",
    "\n",
    "    end = time.time()\n",
    "    for epoch in tqdm(range(args['epochs'])):\n",
    "        epoches += [epoch]\n",
    "\n",
    "        # train for one epoch\n",
    "        acc_train_per, loss_train_per, output_train_per, target_train_per = train(train_loader, model, criterion, optimizer, lr_scheduler, args)\n",
    "        \n",
    "        acc_train += [acc_train_per]\n",
    "        loss_train += [loss_train_per]\n",
    "        # calculate metric\n",
    "        # print(len(target_train_per),len(output_train_per))\n",
    "        \n",
    "        # ohe_train = torch.nn.functional.one_hot(target_train_per, num_classes=15)\n",
    "        \n",
    "        ba_value_train = balanced_accuracy_score(target_train_per, output_train_per)\n",
    "        # auc_pr_value_train = average_precision_score(target_train_per, output_train_per)\n",
    "        #new code\n",
    "        ba_train += [ba_value_train]\n",
    "        # pr_train  += [auc_pr_value_train]\n",
    "\n",
    "        msg = f'TRAIN, epoch {epoch}, train_loss {loss_train_per}, train_bacc {ba_value_train}'\n",
    "\n",
    "        print(f'TRAIN, epoch {epoch}, train_loss {loss_train_per:.5f}, train_bacc {ba_value_train:.5f}')\n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        acc_val_per, loss_val_per, output_val_per, target_val_per = validate(val_loader, model, criterion, args)\n",
    "\n",
    "        acc_val  += [acc_val_per]\n",
    "        loss_val += [loss_val_per]\n",
    "        #calculate metric\n",
    "        # calculate metric\n",
    "        # print(len(target_val_per),len(output_val_per))\n",
    "        ba_value_val = balanced_accuracy_score(target_val_per, output_val_per)\n",
    "        # auc_pr_value_val = average_precision_score(target_val_per, output_val_per)\n",
    "        #new code\n",
    "\n",
    "        msg = f'VAL, epoch {epoch}, val_loss {loss_val_per}, val_bacc {ba_value_val}'\n",
    "        \n",
    "        print(f'VAL, epoch {epoch}, val_loss {loss_val_per:.5f}, val_bacc {ba_value_val:.5f}')\n",
    "        \n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "        \n",
    "        \n",
    "        # remember best acc\n",
    "        best_acc1 = max(acc_val_per, best_acc1)\n",
    "        \n",
    "        best_ba = max(ba_value_val, best_ba)\n",
    "        \n",
    "        # best_pr  = max(auc_pr_value_val, best_pr)\n",
    "        \n",
    "    #     wandb.log({\"train_loss\": loss_train_per, \"train_ba\": ba_value_train, \"val_loss\": loss_val_per, \"val_ba\": ba_value_val, \"best_val_ba\": best_ba})\n",
    "        \n",
    "    # wandb.finish()\n",
    "    # measure elapsed time\n",
    "    dataset_time.update(time.time() - end)\n",
    "\n",
    "    # log & print the best_acc\n",
    "    msg = f'\\n\\n * BEST_ACC: {best_acc1}\\n * TIME: {dataset_time}\\n'\n",
    "    log_msg(msg, log_file)\n",
    "\n",
    "    print(f' * best_acc1: {best_acc1}, best_ba: {best_ba}')\n",
    "    print(f' * time: {dataset_time}')\n",
    "    print('****************************************************')\n",
    "\n",
    "\n",
    "    # collect cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, lr_scheduler, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    \n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for count, (data, label) in enumerate(train_loader):\n",
    "\n",
    "        # data in cuda\n",
    "        data = data.cuda(args['gpu']).type(torch.float)\n",
    "        label = label.cuda(args['gpu']).type(torch.long)\n",
    "\n",
    "        # compute output\n",
    "        output = model(data)\n",
    "        \n",
    "        # print(output, label)\n",
    "    \n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1 = accuracy(output, label, topk=(1, 1))\n",
    "        \n",
    "        output_np = np.argmax(torch.softmax(output, dim=1).detach().cpu().numpy(), axis=1).tolist()\n",
    "        \n",
    "        target_np = label.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # print(output_np, target_np)\n",
    "        # break\n",
    "        \n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(acc1[0], data.size(0))\n",
    "        \n",
    "        # met_roc.update(roc, data.size(0))\n",
    "        # met_pr.update(pr, data.size(0))\n",
    "        output_list += output_np\n",
    "        target_list += target_np\n",
    "\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    lr_scheduler.step(top1.avg)\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (data, label) in enumerate(val_loader):\n",
    "            if args['gpu'] is not None:\n",
    "                data = data.cuda(args['gpu'], non_blocking=True).type(torch.float)\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda(args['gpu'], non_blocking=True).type(torch.long)\n",
    "\n",
    "            # compute output\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            output_np = np.argmax(torch.softmax(output, dim=1).detach().cpu().numpy(), axis=1).tolist()\n",
    "            target_np = label.detach().cpu().numpy().tolist()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1 = accuracy(output, label, topk=(1, 1))\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(acc1[0], data.size(0))\n",
    "            \n",
    "            output_list += output_np\n",
    "            target_list += target_np\n",
    "            \n",
    "            # met_roc.update(roc, data.size(0))\n",
    "            # met_pr.update(pr, data.size(0))\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265cb92e-c573-4e83-a10e-791712ca78d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "features / nodes 231 total time graphs 288 classes 15\n",
      "Using cudnn.benchmark.\n",
      "****************************************************\n",
      "Dataset:  Phenotyping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc0c45e91e342f591760f84cd082243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN, epoch 0, train_loss 2.32709, train_bacc 0.16081\n",
      "VAL, epoch 0, val_loss 1.97111, val_bacc 0.24625\n",
      "TRAIN, epoch 1, train_loss 2.07457, train_bacc 0.22615\n",
      "VAL, epoch 1, val_loss 1.88447, val_bacc 0.26103\n",
      "TRAIN, epoch 2, train_loss 1.95790, train_bacc 0.26158\n",
      "VAL, epoch 2, val_loss 1.70149, val_bacc 0.32537\n",
      "TRAIN, epoch 3, train_loss 1.85235, train_bacc 0.30036\n",
      "VAL, epoch 3, val_loss 1.64354, val_bacc 0.34007\n",
      "TRAIN, epoch 4, train_loss 1.77951, train_bacc 0.31675\n",
      "VAL, epoch 4, val_loss 1.63168, val_bacc 0.35663\n",
      "TRAIN, epoch 5, train_loss 1.71758, train_bacc 0.33951\n",
      "VAL, epoch 5, val_loss 1.58660, val_bacc 0.37689\n",
      "TRAIN, epoch 6, train_loss 1.64215, train_bacc 0.36050\n",
      "VAL, epoch 6, val_loss 1.60119, val_bacc 0.35869\n",
      "TRAIN, epoch 7, train_loss 1.57910, train_bacc 0.39054\n",
      "VAL, epoch 7, val_loss 1.58225, val_bacc 0.37758\n",
      "TRAIN, epoch 8, train_loss 1.51760, train_bacc 0.41382\n",
      "VAL, epoch 8, val_loss 1.55885, val_bacc 0.39860\n",
      "TRAIN, epoch 9, train_loss 1.45522, train_bacc 0.46181\n",
      "VAL, epoch 9, val_loss 1.55207, val_bacc 0.37384\n",
      "TRAIN, epoch 10, train_loss 1.36622, train_bacc 0.48358\n",
      "VAL, epoch 10, val_loss 1.62361, val_bacc 0.37426\n",
      "TRAIN, epoch 11, train_loss 1.29099, train_bacc 0.52838\n",
      "VAL, epoch 11, val_loss 1.61731, val_bacc 0.38526\n",
      "TRAIN, epoch 12, train_loss 1.22624, train_bacc 0.55440\n",
      "VAL, epoch 12, val_loss 1.62074, val_bacc 0.40189\n",
      "TRAIN, epoch 13, train_loss 1.16734, train_bacc 0.58743\n",
      "VAL, epoch 13, val_loss 1.71842, val_bacc 0.38658\n",
      "TRAIN, epoch 14, train_loss 1.10963, train_bacc 0.61620\n",
      "VAL, epoch 14, val_loss 1.66150, val_bacc 0.37318\n",
      "TRAIN, epoch 15, train_loss 1.05147, train_bacc 0.65071\n",
      "VAL, epoch 15, val_loss 1.75829, val_bacc 0.39445\n",
      "TRAIN, epoch 16, train_loss 0.96743, train_bacc 0.67741\n",
      "VAL, epoch 16, val_loss 1.83113, val_bacc 0.37780\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_work(args)\n",
      "Cell \u001b[0;32mIn[11], line 99\u001b[0m, in \u001b[0;36mmain_work\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     96\u001b[0m epoches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [epoch]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m acc_train_per, loss_train_per, output_train_per, target_train_per \u001b[38;5;241m=\u001b[39m train(train_loader, model, criterion, optimizer, lr_scheduler, args)\n\u001b[1;32m    101\u001b[0m acc_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [acc_train_per]\n\u001b[1;32m    102\u001b[0m loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [loss_train_per]\n",
      "Cell \u001b[0;32mIn[11], line 182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, lr_scheduler, args)\u001b[0m\n\u001b[1;32m    177\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m count, (data, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# data in cuda\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    183\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mcuda(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# compute output\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_work(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7b3fb-cdbe-463e-9930-3e07a5d373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ignite metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee5b71-df18-40ce-8306-46d25381df1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04268a-6e56-4c9a-af62-ef7a51a2c51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b59bea-155e-48ff-ad28-144da52e2b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d89b1-02ae-4bde-855d-9a7afb3fb29d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de46b36-bbed-4648-b307-08e36812fa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd294a-20b3-4102-a719-ad53f428a282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b77c3-2735-4cb4-b2df-515ec8cdf3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
