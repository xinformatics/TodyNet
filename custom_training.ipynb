{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d6794b-cc66-4c9d-a866-874b7955b8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from net import GNNStack\n",
    "from utils import AverageMeter, accuracy, log_msg, get_default_train_val_test_loader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d78d486-d270-4d1d-acd3-cb2bcbe80351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dsid = \"Mortality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41368305-21e3-4e1f-b14b-7fdc3d09b2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get dataset from .pt\n",
    "# data_train = torch.load(f'data/UCR/{dsid}/X_train.pt')\n",
    "# data_val = torch.load(f'data/UCR/{dsid}/X_valid.pt')\n",
    "# label_train = torch.load(f'data/UCR/{dsid}/y_train.pt')\n",
    "# label_val = torch.load(f'data/UCR/{dsid}/y_valid.pt')\n",
    "\n",
    "# # init [num_variables, seq_length, num_classes]\n",
    "# num_nodes = data_val.size(-2)\n",
    "\n",
    "# seq_length = data_val.size(-1)\n",
    "\n",
    "# num_classes = len(torch.bincount(label_val.type(torch.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c149f953-9867-4376-b929-3ca657950f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_train.shape, data_val.shape, label_train.shape, label_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bdf7f8-10c6-4d59-90ee-d7dea4cb4996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_nodes ### each feature is a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6868d1-8e9f-4c2e-9d54-9f64758bf570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seq_length ### sequence length: number of total graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41610454-0c8c-4a5a-ba43-8bec54701f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_classes ### ri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608f5b7-259a-4fea-be59-b90be979c3b9",
   "metadata": {},
   "source": [
    "## aruguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e2de2d-8c0d-4ab8-8090-5e5621f13034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'arch': 'dyGIN2d', #what other models I can put here?? dyGCN2d\n",
    "    'dataset': 'Mortality',\n",
    "    'num_layers': 3,  # the number of GNN layers\n",
    "    'groups': 4,  # the number of time series groups (num_graphs)\n",
    "    'pool_ratio': 0.2,  # the ratio of pooling for nodes\n",
    "    'kern_size': [9,5,3],  # list of time conv kernel size for each layer\n",
    "    'in_dim': 64,  # input dimensions of GNN stacks\n",
    "    'hidden_dim': 128,  # hidden dimensions of GNN stacks\n",
    "    'out_dim': 256,  # output dimensions of GNN stacks\n",
    "    'workers': 0,  # number of data loading workers\n",
    "    'epochs': 2000,  # number of total epochs to run\n",
    "    'batch_size': 16,  # mini-batch size, this is the total batch size of all GPUs\n",
    "    'val_batch_size': 16,  # validation batch size\n",
    "    'lr': 1e-4,  # initial learning rate\n",
    "    'weight_decay': 1e-4,  # weight decay\n",
    "    'evaluate': False,  # evaluate model on validation set\n",
    "    'seed': 42,  # seed for initializing training\n",
    "    'gpu': 0,  # GPU id to use\n",
    "    'use_benchmark': True,  # use benchmark\n",
    "    'tag': 'date'  # the tag for identifying the log and model files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c31c8a0-dcb1-4e0c-b44e-36bb9bd8f4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TensorDataset(data_train, label_train)\n",
    "# val_dataset   = TensorDataset(data_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f6c801-7a9f-4f8a-9cbf-362febcf0683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=args['batch_size'],shuffle=True, num_workers=args['workers'], pin_memory=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args['val_batch_size'], shuffle=False,num_workers=args['workers'],pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a648e31-70d5-4f32-b03a-6fd95b5211e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # args = parser.parse_args()\n",
    "    \n",
    "#     # args.kern_size = [ int(l) for l in args.kern_size.split(\",\") ]\n",
    "\n",
    "#     # if args.seed is not None:\n",
    "#     random.seed(args['seed'])\n",
    "#     torch.manual_seed(args['seed'])\n",
    "\n",
    "#     main_work(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "991c1bc0-5e2d-4ea2-9691-3e2fac259bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_work(args):\n",
    "    \n",
    "    random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    \n",
    "    \n",
    "    # init acc\n",
    "    best_acc1 = 0\n",
    "    \n",
    "    if args['tag'] == 'date':\n",
    "        local_date = time.strftime('%m.%d', time.localtime(time.time()))\n",
    "        args['tag'] = local_date\n",
    "\n",
    "    log_file = 'log/{}_gpu{}_{}_{}_exp.txt'.format(args['tag'], args['gpu'], args['arch'], args['dataset'])\n",
    "\n",
    "    if args['gpu'] is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args['gpu']))\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    train_loader, val_loader, num_nodes, seq_length, num_classes = get_default_train_val_test_loader(args)\n",
    "    \n",
    "    print(num_nodes,seq_length,num_classes)\n",
    "    \n",
    "    # training model from net.py\n",
    "    model = GNNStack(gnn_model_type=args['arch'], num_layers=args['num_layers'], \n",
    "                     groups=args['groups'], pool_ratio=args['pool_ratio'], kern_size=args['kern_size'], \n",
    "                     in_dim=args['in_dim'], hidden_dim=args['hidden_dim'], out_dim=args['out_dim'], \n",
    "                     seq_len=seq_length, num_nodes=num_nodes, num_classes=num_classes)\n",
    "\n",
    "    # print & log\n",
    "    log_msg('epochs {}, lr {}, weight_decay {}'.format(args['epochs'], args['lr'], args['weight_decay']), log_file)\n",
    "\n",
    "\n",
    "    # determine whether GPU or not\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Warning! Using CPU!!!\")\n",
    "    elif args.gpu is not None:\n",
    "        torch.cuda.set_device(args['gpu'])\n",
    "\n",
    "        # collect cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = model.cuda(args['gpu'])\n",
    "        if args['use_benchmark']:\n",
    "            cudnn.benchmark = True\n",
    "        print('Using cudnn.benchmark.')\n",
    "    else:\n",
    "        print(\"Error! We only have one gpu!!!\")\n",
    "\n",
    "\n",
    "    # define loss function(criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(args['gpu'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=50, verbose=True)\n",
    "\n",
    "    # validation\n",
    "    if args['evaluate']:\n",
    "        validate(val_loader, model, criterion, args)\n",
    "        return\n",
    "\n",
    "    # train & valid\n",
    "    print('****************************************************')\n",
    "    print('Dataset: ', args['dataset'])\n",
    "\n",
    "    dataset_time = AverageMeter('Time', ':6.3f')\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    epoches = []\n",
    "\n",
    "    end = time.time()\n",
    "    for epoch in tqdm.notebook(range(args['epochs'])):\n",
    "        epoches += [epoch]\n",
    "\n",
    "        # train for one epoch\n",
    "        acc_train_per, loss_train_per = train(train_loader, model, criterion, optimizer, lr_scheduler, args)\n",
    "        \n",
    "        acc_train += [acc_train_per]\n",
    "        loss_train += [loss_train_per]\n",
    "\n",
    "        msg = f'TRAIN, epoch {epoch}, loss {loss_train_per}, acc {acc_train_per}'\n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc_val_per, loss_val_per = validate(val_loader, model, criterion, args)\n",
    "\n",
    "        acc_val += [acc_val_per]\n",
    "        loss_val += [loss_val_per]\n",
    "\n",
    "        msg = f'VAL, loss {loss_val_per}, acc {acc_val_per}'\n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "        # remember best acc\n",
    "        best_acc1 = max(acc_val_per, best_acc1)\n",
    "\n",
    "\n",
    "    # measure elapsed time\n",
    "    dataset_time.update(time.time() - end)\n",
    "\n",
    "    # log & print the best_acc\n",
    "    msg = f'\\n\\n * BEST_ACC: {best_acc1}\\n * TIME: {dataset_time}\\n'\n",
    "    log_msg(msg, log_file)\n",
    "\n",
    "    print(f' * best_acc1: {best_acc1}')\n",
    "    print(f' * time: {dataset_time}')\n",
    "    print('****************************************************')\n",
    "\n",
    "\n",
    "    # collect cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, lr_scheduler, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc', ':6.2f')\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for count, (data, label) in enumerate(train_loader):\n",
    "\n",
    "        # data in cuda\n",
    "        data = data.cuda(args.gpu).type(torch.float)\n",
    "        label = label.cuda(args.gpu).type(torch.long)\n",
    "\n",
    "        # compute output\n",
    "        output = model(data)\n",
    "    \n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1 = accuracy(output, label, topk=(1, 1))\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(acc1[0], data.size(0))\n",
    "\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    lr_scheduler.step(top1.avg)\n",
    "\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (data, label) in enumerate(val_loader):\n",
    "            if args.gpu is not None:\n",
    "                data = data.cuda(args.gpu, non_blocking=True).type(torch.float)\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda(args.gpu, non_blocking=True).type(torch.long)\n",
    "\n",
    "            # compute output\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1 = accuracy(output, label, topk=(1, 1))\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(acc1[0], data.size(0))\n",
    "\n",
    "    return top1.avg, losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "265cb92e-c573-4e83-a10e-791712ca78d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: 0 for training\n",
      "231 288 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_work(args)\n",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m, in \u001b[0;36mmain_work\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m GNNStack(gnn_model_type\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124march\u001b[39m\u001b[38;5;124m'\u001b[39m], num_layers\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     27\u001b[0m                  groups\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m'\u001b[39m], pool_ratio\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m], kern_size\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkern_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     28\u001b[0m                  in_dim\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], out_dim\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     29\u001b[0m                  seq_len\u001b[38;5;241m=\u001b[39mseq_length, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# print & log\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m log_msg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, lr \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, weight_decay \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mepochs, args\u001b[38;5;241m.\u001b[39mlr, args\u001b[38;5;241m.\u001b[39mweight_decay), log_file)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# determine whether GPU or not\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'epochs'"
     ]
    }
   ],
   "source": [
    "main_work(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7b3fb-cdbe-463e-9930-3e07a5d373a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
